# Code_Fix

**Generated**: 2025-12-30T05:01:39.663330
**Novelty Score**: 0.39
**Source File**: /home/josh/str/projects/codedreamer/codedreamer/conductor.py

---

 ## Analysis

### 1. Current State Assessment
- **What is this code doing?**
  - The `_handle_code` method in the `Conductor` class handles a code generation request by constructing a prompt from the provided context and request, sending it to an LLM (Language Model) via the `orchestra.code` method, logging the response and duration, and storing the generated code in a graph.
- **What patterns/paradigms is it using?**
  - The method uses string interpolation to construct the prompt, measures the time taken for the code generation request, logs the details of the operation, and stores the result in a graph. It also follows a procedural pattern within the method.

### 2. Identified Issues
- **String Interpolation**:
  - **Function**: `prompt = f"Generate code for this request.\n\nContext:\n{context}\n\nRequest: {request}\n\nCode:"`
  - **Issue**: Using multi-line strings and string interpolation can make the prompt less readable and harder to maintain. It also makes it difficult to modify or extend the prompt structure.
- **Hard-Coded Temperature**:
  - **Function**: `response = self.orchestra.code(prompt, temperature=0.3)`
  - **Issue**: The temperature parameter is hard-coded, which can limit flexibility and adaptability of the code generation process. Different tasks might require different temperature settings for better results.
- **Fixed Content Length**:
  - **Function**: `self.graph.add_node(content=response[:500], node_type=NodeType.CODE, metadata={"request": request[:100]})`
  - **Issue**: Truncating the content and metadata to fixed lengths can result in loss of important information. This might be problematic if the generated code or request is longer than the specified limits.
- **Lack of Error Handling**:
  - **Function**: `response = self.orchestra.code(prompt, temperature=0.3)`
  - **Issue**: There is no error handling for the LLM call. If the LLM fails to generate a response, the method will either return an empty string or raise an unhandled exception, which can lead to unexpected behavior.
- **Logging Details**:
  - **Function**: `self.scratch.add_step(action="code", input_text=request, output_text=response, model_used="coder", duration_ms=duration, task_id=task.id)`
  - **Issue**: The logging details are hardcoded and might need to be more dynamic or configurable. This can make the method less flexible if different types of logs are required.

### 3. Proposed Improvement
- **Refactor Prompt Construction**:
  - Use a template string for the prompt to improve readability and maintainability.
  - Example: 
    ```python
    prompt_template = """
    Generate code for this request.

    Context:
    {context}

    Request: {request}

    Code:
    """
    prompt = prompt_template.format(context=context, request=request)
    ```
- **Make Temperature Configurable**:
  - Introduce a configuration setting or method parameter to allow the temperature to be adjusted.
  - Example:
    ```python
    def _handle_code(self, request: str, context: str, task: Task, temperature: float = 0.3) -> str:
        ...
        response = self.orchestra.code(prompt, temperature=temperature)
        ...
    ```
- **Dynamic Content Length**:
  - Use a configurable maximum length for content and metadata to avoid hard-coding limits.
  - Example:
    ```python
    def _handle_code(self, request: str, context: str, task: Task, temperature: float = 0.3, max_content_length: int = 500, max_metadata_length: int = 100) -> str:
        ...
        self.graph.add_node(
            content=response[:max_content_length],
            node_type=NodeType.CODE,
            metadata={"request": request[:max_metadata_length]},
        )
        ...
    ```
- **Add Error Handling**:
  - Implement a try-except block to handle potential errors from the LLM call.
  - Example:
    ```python
    try:
        response = self.orchestra.code(prompt, temperature=temperature)
    except Exception as e:
        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=f"Error: {str(e)}",
            model_used="coder",
            duration_ms=int((time.time() - start) * 1000),
            task_id=task.id,
        )
        return f"Error: {str(e)}"
    ```
- **Dynamic Logging**:
  - Introduce a method to dynamically construct the logging details based on the task or other parameters.
  - Example:
    ```python
    def _log_code_step(self, request: str, response: str, duration_ms: int, task_id: str):
        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration_ms,
            task_id=task_id,
        )
    ```

### 4. Implementation Strategy
1. **Refactor Prompt Construction**:
   - Replace the current string interpolation with a template string.
2. **Make Temperature Configurable**:
   - Add a `temperature` parameter to the `_handle_code` method with a default value of 0.3.
3. **Dynamic Content Length**:
   - Add `max_content_length` and `max_metadata_length` parameters to the `_handle_code` method with default values of 500 and 100, respectively.
4. **Add Error Handling**:
   - Wrap the LLM call in a try-except block and log any errors that occur.
5. **Dynamic Logging**:
   - Extract the logging logic into a separate `_log_code_step` method.

### Step-by-Step Approach
1. **Update Method Signature**:
   ```python
   def _handle_code(self, request: str, context: str, task: Task, temperature: float = 0.3, max_content_length: int = 500, max_metadata_length: int = 100) -> str:
   ```
2. **Refactor Prompt Construction**:
   ```python
   prompt_template = """
   Generate code for this request.

   Context:
   {context}

   Request: {request}

   Code:
   """
   prompt = prompt_template.format(context=context, request=request)
   ```
3. **Add Error Handling**:
   ```python
   try:
       response = self.orchestra.code(prompt, temperature=temperature)
   except Exception as e:
       self._log_code_step(request, f"Error: {str(e)}", int((time.time() - start) * 1000), task.id)
       return f"Error: {str(e)}"
   ```
4. **Dynamic Content Length**:
   ```python
   self.graph.add_node(
       content=response[:max_content_length],
       node_type=NodeType.CODE,
       metadata={"request": request[:max_metadata_length]},
   )
   ```
5. **Extract Logging Logic**:
   ```python
   def _log_code_step(self, request: str, response: str, duration_ms: int, task_id: str):
       self.scratch.add_step(
           action="code",
           input_text=request,
           output_text=response,
           model_used="coder",
           duration_ms=duration_ms,
           task_id=task_id,
       )
   ```

### Tests to Validate the Change
1. **Unit Test for Prompt Construction**:
   - Verify that the prompt is constructed correctly with different `context` and `request` values.
2. **Unit Test for Temperature Parameter**:
   - Ensure that the `temperature` parameter is passed correctly to the LLM call.
3. **Unit Test for Dynamic Content Length**:
   - Check that the content and metadata are truncated to the specified lengths.
4. **Unit Test for Error Handling**:
   - Simulate an error from the LLM call and verify that the method returns an error message and logs it correctly.
5. **Integration Test for Logging**:
   - Verify that the `_log_code_step` method logs the correct details.

By implementing these changes, the code will be more maintainable, flexible, and robust, with better error handling and logging capabilities. ## Analysis

### 1. Current State Assessment
- **What is this code doing?**
  - The `_handle_code` method in the `Conductor` class handles a code generation request by constructing a prompt from the provided context and request, sending it to an LLM (Language Model) via the `orchestra.code` method, logging the response and duration, and storing the generated code in a graph.
- **What patterns/paradigms is it using?**
  - The method uses string interpolation to construct the prompt, measures the time taken for the code generation request, logs the details of the operation, and stores the result in a graph. It follows a procedural pattern within the method.

### 2. Identified Issues
- **String Interpolation**:
  - **Function**: `prompt = f"Generate code for this request.\n\nContext:\n{context}\n\nRequest: {request}\n\nCode:"`
  - **Issue**: Using multi-line strings and string interpolation can make the prompt less readable and harder to maintain. It also makes it difficult to modify or extend the prompt structure.
- **Hard-Coded Temperature**:
  - **Function**: `response = self.orchestra.code(prompt, temperature=0.3)`
  - **Issue**: The temperature parameter is hard-coded, which can limit flexibility and adaptability of the code generation process. Different tasks might require different temperature settings for better results.
- **Fixed Content Length**:
  - **Function**: `self.graph.add_node(content=response[:500], node_type=NodeType.CODE, metadata={"request": request[:100]})`
  - **Issue**: Truncating the content and metadata to fixed lengths can result in loss of important information. This might be problematic if the generated code or request is longer than the specified limits.
- **Lack of Error Handling**:
  - **Function**: `response = self.orchestra.code(prompt, temperature=0.3)`
  - **Issue**: There is no error handling for the LLM call. If the LLM fails to generate a response, the method will either return an empty string or raise an unhandled exception, which can lead to unexpected behavior.
- **Logging Details**:
  - **Function**: `self.scratch.add_step(action="code", input_text=request, output_text=response, model_used="coder", duration_ms=duration, task_id=task.id)`
  - **Issue**: The logging details are hardcoded and might need to be more dynamic or configurable. This can make the method less flexible if different types of logs are required.

### 3. Proposed Improvement
- **Refactor Prompt Construction**:
  - Use a template string for the prompt to improve readability and maintainability.
  - Example:
    ```python
    prompt_template = """
    Generate code for this request.

    Context:
    {context}

    Request: {request}

    Code:
    """
    prompt = prompt_template.format(context=context, request=request)
    ```
- **Make Temperature Configurable**:
  - Introduce a configuration setting or method parameter to allow the temperature to be adjusted.
  - Example:
    ```python
    def _handle_code(self, request: str, context: str, task: Task, temperature: float = 0.3) -> str:
        ...
        response = self.orchestra.code(prompt, temperature=temperature)
        ...
    ```
- **Dynamic Content Length**:
  - Use a configurable maximum length for content and metadata to avoid hard-coding limits.
  - Example:
    ```python
    def _handle_code(self, request: str, context: str, task: Task, temperature: float = 0.3, max_content_length: int = 500, max_metadata_length: int = 100) -> str:
        ...
        self.graph.add_node(
            content=response[:max_content_length],
            node_type=NodeType.CODE,
            metadata={"request": request[:max_metadata_length]},
        )
        ...
    ```
- **Add Error Handling**:
  - Implement a try-except block to handle potential errors from the LLM call.
  - Example:
    ```python
    try:
        response = self.orchestra.code(prompt, temperature=temperature)
    except Exception as e:
        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=f"Error: {str(e)}",
            model_used="coder",
            duration_ms=int((time.time() - start) * 1000),
            task_id=task.id,
        )
        return f"Error: {str(e)}"
    ```
- **Dynamic Logging**:
  - Introduce a method to dynamically construct the logging details based on the task or other parameters.
  - Example:
    ```python
    def _log_code_step(self, request: str, response: str, duration_ms: int, task_id: str):
        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration_ms,
            task_id=task_id,
        )
    ```

### 4. Implementation Strategy
1. **Refactor Prompt Construction**:
   - Replace the current string interpolation with a template string.
2. **Make Temperature Configurable**:
   - Add a `temperature` parameter to the `_handle_code` method with a default value of 0.3.
3. **Dynamic Content Length**:
   - Add `max_content_length` and `max_metadata_length` parameters to the `_handle_code` method with default values of 500 and 100, respectively.
4. **Add Error Handling**:
   - Wrap the LLM call in a try-except block and log any errors that occur.
5. **Dynamic Logging**:
   - Extract the logging logic into a separate `_log_code_step` method.

### Step-by-Step Approach
1. **Update Method Signature**:
   ```python
   def _handle_code(self, request: str, context: str, task: Task, temperature: float = 0.3, max_content_length: int = 500, max_metadata_length: int = 100) -> str:
   ```
2. **Refactor Prompt Construction**:
   ```python
   prompt_template = """
   Generate code for this request.

   Context:
   {context}

   Request: {request}

   Code:
   """
   prompt = prompt_template.format(context=context, request=request)
   ```
3. **Add Error Handling**:
   ```python
   try:
       response = self.orchestra.code(prompt, temperature=temperature)
   except Exception as e:
       self._log_code_step(request, f"Error: {str(e)}", int((time.time() - start) * 1000), task.id)
       return f"Error: {str(e)}"
   ```
4. **Dynamic Content Length**:
   ```python
   self.graph.add_node(
       content=response[:max_content_length],
       node_type=NodeType.CODE,
       metadata={"request": request[:max_metadata_length]},
   )
   ```
5. **Extract Logging Logic**:
   ```python
   def _log_code_step(self, request: str, response: str, duration_ms: int, task_id: str):
       self.scratch.add_step(
           action="code",
           input_text=request,
           output_text=response,
           model_used="coder",
           duration_ms=duration_ms,
           task_id=task.id,
       )
   ```

### Tests to Validate the Change
1. **Unit Test for Prompt Construction**:
   - Verify that the prompt is constructed correctly with different `context` and `request` values.
2. **Unit Test for Temperature Parameter**:
   - Ensure that the `temperature` parameter is passed correctly to the LLM call.
3. **Unit Test for Dynamic Content Length**:
   - Check that the content and metadata are truncated to the specified lengths.
4. **Unit Test for Error Handling**:
   - Simulate an error from the LLM call and verify that the method returns an error message and logs it correctly.
5. **Integration Test for Logging**:
   - Verify that the `_log_code_step` method logs the correct details.

By implementing these changes, the code will be more maintainable, flexible, and robust, with better error handling and logging capabilities. This will improve the overall quality and reliability of the `Conductor` class. ## Analysis

### 1. Current State Assessment
- **What is this code doing?**
  - The `_handle_code` method in the `Conductor` class handles a code generation request by constructing a prompt from the provided context and request, sending it to an LLM (Language Model) via the `orchestra.code` method, logging the response and duration, and storing the generated code in a graph.
- **What patterns/paradigms is it using?**
  - The method uses string interpolation to construct the prompt, measures the time taken for the code generation request, logs the details of the operation, and stores the result in a graph. It follows a procedural pattern within the method.

### 2. Identified Issues
- **String Interpolation**:
  - **Function**: `prompt = f"Generate code for this request.\n\nContext:\n{context}\n\nRequest: {request}\n\nCode:"`
  - **Issue**: Using multi-line strings and string interpolation can make the prompt less readable and harder to maintain. It also makes it difficult to modify or extend the prompt structure.
- **Hard-Coded Temperature**:
  - **Function**: `response = self.orchestra.code(prompt, temperature=0.3)`
  - **Issue**: The temperature parameter is hard-coded, which can limit flexibility and adaptability of the code generation process. Different tasks might require different temperature settings for better results.
- **Fixed Content Length**:
  - **Function**: `self.graph.add_node(content=response[:500], node_type=NodeType.CODE, metadata={"request": request[:100]})`
  - **Issue**: Truncating the content and metadata to fixed lengths can result in loss of important information. This might be problematic if the generated code or request is longer than the specified limits.
- **Lack of Error Handling**:
  - **Function**: `response = self.orchestra.code(prompt, temperature=0.3)`
  - **Issue**: There is no error handling for the LLM call. If the LLM fails to generate a response, the method will either return an empty string or raise an unhandled exception, which can lead to unexpected behavior.
- **Logging Details**:
  - **Function**: `self

## Suggested Implementation

```python

from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",
            duration_ms=duration,
            task_id=task.id,
        )

        # Store in graph
        self.graph.add_node(
            content=response[:500],
            node_type=NodeType.CODE,
            metadata={"request": request[:100]},
        )

        return response

    def _construct_prompt(self, request: str, context: str) -> str:
        """Construct the prompt for code generation."""
        return f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""
```
```python
from typing import Any, Dict, Optional
import time

class Conductor:
    def __init__(self, orchestra: Any, scratch: Any, graph: Any):
        self.orchestra = orchestra
        self.scratch = scratch
        self.graph = graph

    def _handle_code(self, request: str, context: str, task: Any) -> str:
        """Handle a code generation request."""
        prompt = self._construct_prompt(request, context)
        start_time = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start_time) * 1000)

        self.scratch.add_step(
            action="code",
            input_text=request,
            output_text=response,
            model_used="coder",

```

---

## Seed Context

```
# method: Conductor._handle_code
# Handle a code generation request.
    def _handle_code(self, request: str, context: str, task: Task) -> str:
        """Handle a code generation request."""
        prompt = f"""Generate code for this request.

Context:
{context}

Request: {request}

Code:"""

        start = time.time()
        response = self.orchestra.code(prompt, temperature=0.3)
        duration = int((time.time() - start) * 1000)

        self.scratch.add_step(
            action="code",

```
